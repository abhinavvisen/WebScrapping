
Database Systems
27 September 2016
Data Mining and Web Scrapping
INTRODUCTION AREA
One of the most precious things in the world today is information. It not only has varied uses but it has also evolved as a necessity. People have become dependent on information for all steps in life. One of the vast sources of information is the World Wide Web. It represents a universal library of information with extensive details in every field. Websites today take advantage of this huge repository of knowledge and information to suit their needs. But collecting information from the web and putting it in your site is like taking a bucket of water from the ocean and throwing it back to the ocean. Hence a need for an effective mechanism to get data from the web automatically and storing it in an organised fashion arouse. This led to the development of the technique of “web scraping”. Scraping is a simple technique that retrieves data from the raw HTML script of a website.  Powerful languages like Python and GO enable us to implement just that. 


BEAUTIFULSOUP
One of the most common setup for web scraping is done using Python and Beautiful soup and Python module. The basic setup would require the UrlLib Library and BeautifulSoup Python library. 
UrlLib2:  It is a Python module used for calling urls and retrieving information from the URL.
BeautifulSoup: It is a tool that helps extracting data from html script of the webpage. It can extract paragraphs, lists, forms and even put filters. 
BeautifulSoup comes with various flavours or forms based on parsing methods.
These are:
BeautifulSoup
BeautifulStoneSoup
ICantBelieveItsBeautifulSoup
BeautifulSOAP

SEMANTIC SCRAPPING MODEL
The idea of web mining uses the data mining technique for the extraction of information(of any type) from various sources on the web.The extraction  of information requires either processing of information from DOM(Document Object Model) or through the rendering resulting information. In the first method we define a wrapper that extracts only the required information.The second approach is a vision based approach in which we provide more general solution by interpreting similar content types. In this article we will present and discuss about RDF Graphs that helps us to represent HTML Document.In this paper we will discuss about semantic extraction of data from web. The model provides semantic extraction of data in three different levels.
1.Scrapping Service level-This level provides services and applications importance to the end user.Scrapping technology provides access to large amount of data from the web.The pattern at the other end of scrapping services is quite different from the old Semantic web applications.By this we can infer that we need to follow certain rules for the designing of web services:-
	a.Data Identification:- Data which has to be extracted and merged with the other datas is identified. Small websites are used for fragment extraction.
	b.Data Modelling:- An architecture is defined to represent the extracted data.Either existing models are available or new models are defined.The result from this ontology gives rise to an architecture that fits the extracted data.
	c.Generalization of Extractor:-Enough samples must be collected to identify an appropriate extractor.Because this extractor will be responsible for the extraction of large amount of data.The samples will be given to the human 	     administrator or an automated module. Using this one or more extractors are defined for semantic service level.
2.Semantic Scrapping Level:-In this we define a model that links HTML fragments to the semantic web resources. We do this by expressing our model between  RDF and HTML.The summary of the model is given below.
 
3.Syntactic scraping Level:-In this level we get the knowledge of the technologies that we use to get or extract our data from the different web resources.Some of the Scrapping techniques are:-
a.CSS Selectors-Cascading style sheets give visual effect to the HTML elements.HTML elements are linked with the help of CSS Selectors which extract data.
b.XML is another path language which is used as a node selection.
c.URI patterns allows us to select the document on the resource URI patterns.
In a nutshell Syntactic scrapping level allows us to select the data using the visual information such as XML selectors, CSS Selectors etc.
 


1:SYSTEMS AND METHODS FOR IDENTIFYING AND EXTRACTING DATA FROM HTML PAGES:  The above paper is generated by google and is a registered patent under some US code.The above paper gives a general idea how google implements web scraping. The general idea that this paper gives to everyone is that there is a manual process and an automatic process for creating  web scraper. The manual part is usually parsing the model page that is selected and then parse it using the html tags provided.Then Implement a process of retrieving the required data that is acquired by that model. And only comparing the relevant keywords or portions that was involved in first page and see if all the subsequent pages have the same string matched. They use “string matching” algorithms to compare string and regex to disallow repetition of the same data many times,saving time to go through each and every one them.It also limits the implementation by only search the required tags which are essential to the user which manually entered the details to be searched.Once found it would stop looking for alternatives.

METHOD FOR EXTRACTING DATA FROM WEB PAGES: Web scraping technologies in an API world: This article aims at creating 2 essential points in the field of web scraping one is to identify the main observations that come up through an investigative approach which is used in the implementation of a web scraper and also telling about the current tools available and frameworks that are available for scraping can be used for biomedical applications.There are many examples out of which some practical scenarios are discussed involving antimicrobial susceptibility and new drugs.The first sections talks about the scenarios while the second section talks about these scenarios can be put to test,while the second scenarios reflect on the most common tools that could be used for speeding up the now slow process of programming the scrapers, the third part goes into detail with creating so called “pipelines” of information channels that can be used for medical case studies.The last part tells about the situation right now. The use of web robots that mimic the humans presence between servers and humans.It step by step accesses and parses the data.Talking about the fact that most web scrapers used in bioinformatics are made of tools already being used by the community like perl,java,python and libcurl.They include packages like jsoup and beautifulsoup.Then there are frameworks like scrapy for python and web-harvest for java. For programmers who tend to use GUI and not so proficient with code are also heard for desktop based applications, but they tend to have a major drawback in terms of the their limited API calls which can be due the fact that they are mostly commercial distribution.In bioinformatics web scraping is compiled out of public websites that do not provide an API even though their info is public.Example of one such use of web scraping is explained in the above paper is by taking one such target pathogenic specie “Pseudomonas aeruginosa” they use web scraping by using three main databases the antimicrobial peptide databases and the collection of antimicrobial peptides  and the European Committee on Antimicrobial Susceptibility Testing.The first 2 provide data on natural peptides while that last one provIdes data on MIC.This provides a great deal of information about  antimicrobial products related to the above microbe.The output generated  which is to be presented to the user is in the order of peptide names which are essentially for standardizationand promote multi-source data integration.AMP source which include the scientific names of plants or animals.AMP activity  like antibacterial ,antifungal,cancer cells etc.There are specifications in terms of gram-positive and gram-negative bacteria , then the MIC values of antimicrobial products found to target P. aruginosa.This whole workflow implemented is done using jARVEST. The WhichGenes meta server gets us lists of genes responsible of diseases , involved in metabolic pathways, annotated with GO terms, target of microRNAs and so on.WhichGenes meta-server obtaining gene list by querying 3rd party resources.PathJam is another database integration.

WEB SCRAPING MADE SIMPLE WITH SITE SCRAPER :Since the bulk of web’s data is obscured in (X)HTML by a layer of presentation,with different styles for each website.Additionally , these styles change overtime as each website when they are created with new layout. This makes working with data across websites very difficult.This tool presented in the current paper is called SiteScraper.The workflow of site scraper is ----------------------------------  user accesses a website to get some info -->they find a few webpages with that info at a website and give SiteScraper the urls and the info  displayed as a list of strings ---> site scraper parses the webpages to get the strings about the info.---> locations are examined and a model is created.This model generated usually of a  in form of a graph. Site scraper was created for ILIAD which is used for linux troubleshooting from web sources. Site scraper how it sees the web page structure, In one case The content changes  and the structure remains the same ,and in the other case The content stays the same while the structure changes.The way site scraper works is that it requires a small sample set of seed documents, each of which is paired with a list of text chunks.  The text chunks associated with a given seed document and they look out for  the same type of strings present in the seed document.This methodology gives a clear cut  aim for site scraper one is to obtain the well defined data present in those web pages second, it aims to automatically retrain the scraping method even after it has gone through structural changes with relation to the web page itself.

Retrieving media items from multiple social networks : The idea is that there are so many media outlets that accessing data is not a difficult task instead what we can do with data is the question answered by the paper . 
The amount of media present online as mostly images, these images may be duplicated and to interpret these and create stories and condensed forms of galleries and also reduplicated images.The paper is created towards a particular demo on webpage.The webpage assess the performances of the image process reduplication and then we propose a human evaluation of the summary.

Scraping the social: The paper tells how scraping is used in social research using automated capture of online data.How to make data an important part of the productive for social research scraping has the capacity to restructure research in  2 ways. Firstly it’s not exclusive to social research , scraping involves analytic  assumptions.
Semi-Automatic Wrapper Generation for Commercial Web Sources : Semi-automatic wrapper generation tools eases  the task of building structure views over semi- structured web pages.But the wrapper generation techniques presented up to date are unable to properly deal with sources requiring complex navigational sequences for accessing data.In this paper, we present WARGO, a semi-automatic wrapper generation tool, which has been used by non-programmer staff to successfully wrap more than 700 commercial web sources in several industrial applications. We describe our approach for wrapper generation and show the difficulties found with other systems for wrapping this kind of sources.                                                      


Application Area
1). API (Application Programming Interface) s today make extensive use of scraping as a means to provide application developers information in common, usable formats like JSON, XML etc. 
Most of the APIs requires us to agree to their terms and agreements and some even put a limit on maximum calls allowed per day. This may sometimes even come as a paid service. But scraping allows us to do extract data without their consent and knowledge. It also helps in automation of various processes like updating news feeds. One such example of Web Scraping would be the “My VIT” android app of VIT University. It enables students to monitor their attendance, marks and subject details. All these information are in the VIT website but it is not possible for us to check the site every now and then. Hence using a scraping technology, the data is hosted in the server and this can be constantly called from any device to get data. A prior permission from the University is not mandatory for scraping.



2.) Consumer Price Index (CPI)
One useful application of WebScraping can be attributed to the estimation of Inflation and Deflation rates in the market with the help of CPI (Consumer Price Index).The CPI is a simple model that takes regularly used items and its prices in a conceptual collection called as a “basket”. A basket contains articles like toothpaste, milk, egg etc. The basket values for each item are calculated by the difference in price of the item compared to the previous basket. Changes are recorded as percentage. The CPI figure can categorized into either of these two i.e. CPI-W (for Urban Wage Earners and Clerical Workers) and CPI-U (All Urban Consumers). 
The consumer patterns are represented in graphs with different time of the year. I helps predicting the consumers groups for different geographical regions. It also helps in predicting inflation and deflation. Large CPI rises during short interval of time indicates inflations and large downfalls of CPI in short time interval indicates deflations. The data obtained is useful for deciding upon various cash flow mechanism like pension, Medicare or cost of living adjustents  
The Consumer Prices Index (CPI) is produced monthly by the Office for National Statistics (ONS). The index measures the change in price of a fixed. Prices for around 520 of these items are collected by price collectors from stores across the country. The remaining prices are collected centrally through websites, catalogues and by phone. This is often referred to as traditional price collection in index number literature.
A decision was made by the ONS to switch to alternative sources of data as opposed to the regular prices from the stores. Since online marketing is booming and dominating, its relevance in the CPI would reflect a lot on the inflation changes. 
Data Collection and Extraction
ONS’s Big Data pilot for prices developed prototype web scrapers for online supermarket chains like Tesco, Sainsbury and Waitrose, who account for approximately 50% share of the grocery market (according to Cantor World Panel, 2016). These scrapers were programmed in Python using the “Scrappy” module.
Every day at 5.00 am the web scrapers automatically extract prices for 33 items in the CPI basket. While the current local collection item descriptions are tightly defined to ensure it is easy to find comparable replacements, the web scrapers collect prices from a broader range of products because the websites do not distinguish between these criteria. 
The scraper uses the websites own structuring procedure to match the descriptions needed for the basket. Discounts are recorded if it applies for everyone and are taken as exceptions.
Web scraping price undoubtedly offers a number of benefits. It has the potential to reduce collection costs, and increase the frequency and number of products that are represented in the basket. It also has the potential to deepen our understanding of price behaviour through additional and higher frequency indices. 
However, the data are different (which does not imply worse) to traditionally collected price data. This means that there are a number of limitations and caveats that should be applied to the data. These prevent conclusions being drawn on supermarket’s online price behaviour or comparisons to national inflation rates.


3.)SCRAPPY
It is designed to extract data from the website that has multiple pages of semantic structure.The system is usually used as a browser extension and mainly works in three stages to extract the data.In the first stage the user goes to the page and selects the template for the content from which he/she wants to extract data. In the second stage the user selects the set of links that posts towards the content template.In the last stage the user selects an output data and the scrappy system crawls the link provided by the user and extracts the data according to the user template.

1.Template Creation:-The first step to extract data from the web is to create a content template which can only be done by visiting a website with multiple pages of crapping data.In order to initialise a template we must go to the sample web page and begin the template mode.As we enter template mode our system does four thing.
	a.Modification of current page is done in order for the user to select the content template.
	b.Suggestions of the template that help in scrapping.
	c.Highlight the suggested element.When the user hovers the highlighted element becomes blue.
	d.Links are disabled in order to avoid the loss of data.

2.Link Selection:- Once the content template is selected a set of link must be selected which directs towards the content template.When the user moves over a set of links the scrappy highlights the set of similar links for data extraction.

3.Scrapping:-After the completion of above two steps the user simply selects the format of output data(currently CSV and JSON) are supported.After this Scrappy creates threads for the pages the user has given during the link selection process.For each section in the template we created a Path Query which we use to traverse the DOM.Once all the scrapping is complete scrappy compiles the data and displays in the appropriate format.



Data 
URL 
Successfully Scraped 
Comments 
IMDB Movies 
imdb.com 
Title, summary, year, genre, cast, rating, director, etc. 
Success! 
Amazon Music Store 
amazon.com/ music 
Artist ,genre, album, album length, album price, song name 
Successfully scraped data into 3 tables linked by an “id” column. 
Android App Store 
market.android.com 
App name, description, ‘last updated’, app size, app genre, app version, supported android OS, Price, content rating. 
Success! 
Google Finance 
finance. google.com 
Failed, but could scrape name and shares 
Some major fields like market value and daily change could not reliably scrape. Google actively tries to prevent scraping. 
South American countries on Wikipedia 
en.wikipedia.org/ wiki/ South_america 
Failed 
User tried scraping the right-aligned details table of South American countries. Differences in DOM structure for different countries made scrape unreliable. 
LinkedIn connections data 
linkedin.com/ connections 
first name, last name, title, most recent job position, professional experience summary 
This scrape browsed data for all of a user’s connections on LinkedIn. Some anchor elements that were selected in the template didn’t scrape due to Javascript actions LinkedIn associated with the links 
ESPN Basketball Stats 
 
http://espn.go.com/n ba/statistics 
Complete ‘GAME LOG’ and ‘STATS’ tables for each player, player name, 2010-11 season stats 
Some of the subheaders in one of the tables didn’t scrape. This was fixed by the user by comparing Scrappy’s output and the original template. 








Some limitations of web scraped data: 
Accepting to all terms and conditions of the websites involve an agreement of avoiding any kind of scraping and some technologies exists within the company that prevent scraping of their data. 	

All prices are shown as a selling price but we don’t actually know how many of those have been actually bought by the people. Some products might have been never sold but they still would go in the basket data.	

Generally matching of products is done using the name and description. This data could be misleading sometimes because some products may have a completely out of the box name that are not related to its product. 
Sites written in plain HTML are easy to scrape but sites like Facebook and LinkedIn run heavily on javascript and Ajax which can be scraped only with the help of javascript rendering mechanisms. 
















Links and sources :  
http://www.ijircce.com/upload/2015/sacaim/43_710.pdf
Research indices using web scraped data – Office Of National Statistics
http://www.cisstat.com/BigData/CISBigData_04_Eng%20%20ISTAT%20Web%20scraping-HICP.pdf
http://www.iosrjournals.org/iosr-jce/papers/Vol16-issue3/Version-9/I016395460.pdf
jiffy@dit.upm.es, j.blasco@alumunos.upm.es.
Stanford University 1Department of Biomedical Informatics 2Department of Computer Science {amirg, sholbert, nikil}@stanford.edu 
http://digitalcommons.georgiasouthern.edu/honors-theses Part of the Business Administration, Management, and Operations Commons, Databases and Information Systems Commons, and the Management Information Systems Commons.
International Journal of Innovative Research in Computer and Communication Engineering (An ISO 3297: 2007 Certified Organization) Vol. 3, Special Issue 7, October 2015 
Federico Polidoro∗, Riccardo Giannini, Rosanna Lo Conte, Stefano Mosca and Francesca Rossetti1 Istat, Italian National Statistical Institute, Rome, Italy. 



Shuvam Ghosh
Shubham Kumar
Abhinav Visen
